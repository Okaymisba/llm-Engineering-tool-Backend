import os

from dotenv import load_dotenv
from openai import OpenAI

load_dotenv()


def query_deepseek_model(model, question, prompt_context=None, instructions=None, image_data=None, document_data=None):
    """
    Queries the DeepSeek model through the OpenAI Client using the provided parameters.

    This function sends a question to a specified model in the DeepSeek API, along with
    optional context, instructions, image data, and document data. The function streams
    responses, yielding reasoning or content generated by the model in real-time. It also
    tracks and yields metadata about token usage once the process concludes.

    :param model: The name of the DeepSeek model to query.
    :type model: str
    :param question: The question or input string to send to the DeepSeek model.
    :type question: str
    :param prompt_context: Additional relevant context to provide to the model, if available.
    :type prompt_context: Optional[str]
    :param instructions: Instructions or specific directives for the model, if any.
    :type instructions: Optional[str]
    :param image_data: Data related to an image, if relevant for the query.
    :type image_data: Optional[str]
    :param document_data: Data related to a document, if relevant for the query.
    :type document_data: Optional[str]
    :return: Yields a dictionary containing either reasoning, content, or metadata related to token usage.
    :rtype: Generator[dict, None, None]
    """
    client = OpenAI(
        base_url="https://openrouter.ai/api/v1",
        api_key=os.getenv("OPEN_ROUTER_API_KEY"),
    )

    messages = []

    if instructions:
        messages.append({"role": "system", "content": instructions})

    if prompt_context:
        messages.append({"role": "system", "content": f"Here is the context: {prompt_context}"})

    if image_data:
        messages.append({"role": "system", "content": f"Here is the image: {image_data}"})

    if document_data:
        messages.append({"role": "system", "content": f"Here is the document: {document_data}"})

    messages.append({"role": "user", "content": question})

    token_metadata = {"prompt_tokens": None, "completion_tokens": None, "total_tokens": None}

    response = client.chat.completions.create(
        model=f"deepseek/{model}",
        messages=messages,
        stream=True,
    )

    for chunk in response:
        if hasattr(chunk.choices[0].delta, "reasoning") and chunk.choices[0].delta.reasoning:
            yield {"reasoning": chunk.choices[0].delta.reasoning}
        if chunk.choices[0].delta.content:
            yield {"content": chunk.choices[0].delta.content}

        if hasattr(chunk, 'usage') and chunk.usage is not None:
            token_metadata.update({
                "prompt_tokens": getattr(chunk.usage, 'prompt_tokens', None),
                "completion_tokens": getattr(chunk.usage, 'completion_tokens', None),
                "total_tokens": getattr(chunk.usage, 'total_tokens', None)
            })

    yield {"metadata": token_metadata}
